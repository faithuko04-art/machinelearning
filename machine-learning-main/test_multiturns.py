#!/usr/bin/env python3
"""
Multi-turn end-to-end test script to verify the learning pipeline.

Test Flow:
1. Ask Q1 about machine learning
2. Verify unknown detection + Firestore logging
3. Trigger quick learning
4. Ask Q2 (related) and compare responses
5. Provide negative feedback on a response
6. Verify rethinking occurs
7. Ask Q3 (same topic) and verify improvement
"""

import os
import sys
import time
import json
from datetime import datetime

# Add project to path
sys.path.insert(0, '/workspaces/machinelearning/machine-learning-main')

# Use real Firebase Firestore (not local emulator)
if 'FIRESTORE_EMULATOR_HOST' in os.environ:
    del os.environ['FIRESTORE_EMULATOR_HOST']

from brain.knowledge_base import detect_and_log_unknown_words
from brain.db import get_unknown_words, is_known
from logic.advanced_learning import quick_learn_unknowns, deep_learning
from logic.rethink import rethink_and_learn
from services.services import db
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def print_section(title):
    print(f"\n{'='*70}")
    print(f"  {title}")
    print(f"{'='*70}\n")

def test_q1_detection():
    """Test: Ask Q1, verify unknowns are detected and logged"""
    print_section("TEST 1: Unknown Detection (Q1)")
    
    q1 = "Explain supervised learning and neural networks in plain English."
    logger.info(f"Q1: {q1}")
    
    # Simulate unknown detection (what the frontend does on user input)
    detect_and_log_unknown_words(q1)
    
    # Verify unknowns were captured
    unknowns = get_unknown_words()
    unknown_ids = [u.id for u in unknowns]
    logger.info(f"Detected unknowns: {unknown_ids}")
    
    assert len(unknown_ids) > 0, "‚ùå No unknowns detected!"
    assert any('supervised' in u.lower() or 'neural' in u.lower() for u in unknown_ids), \
        "‚ùå Key concepts not detected!"
    
    print(f"‚úÖ Unknown detection working: {len(unknown_ids)} concepts detected")
    print(f"   Sample concepts: {unknown_ids[:5]}")
    return q1, unknown_ids

def test_q2_learning():
    """Test: Trigger quick learning, verify concepts are learned"""
    print_section("TEST 2: Quick Learning Pipeline")
    
    # Get unknown count before learning
    unknowns_before = len(list(get_unknown_words()))
    logger.info(f"Unknowns before learning: {unknowns_before}")
    
    # Trigger quick learning
    logger.info("Triggering quick_learn_unknowns()...")
    result = quick_learn_unknowns()
    
    logger.info(f"Learning result: {json.dumps(result, indent=2)}")
    
    if result['status'] == 'completed':
        learned_count = result.get('learned_count', 0)
        print(f"‚úÖ Quick learning completed: {learned_count} concepts learned")
        
        # Verify some concepts moved from unknown to known
        known_docs = list(db.collection('known_words').limit(3).stream())
        if known_docs:
            known_ids = [k.id for k in known_docs]
            print(f"   Sample learned concepts: {known_ids}")
            return True
    else:
        logger.warning(f"‚ö†Ô∏è  Learning not completed: {result.get('message', 'Unknown reason')}")
        return False

def test_q3_improvement():
    """Test: Ask similar Q but about deeper concepts, verify learned knowledge is used"""
    print_section("TEST 3: Learning Verification (Improved Response)")
    
    q3 = "How do neural networks learn from labeled data? What's backpropagation?"
    logger.info(f"Q3: {q3}")
    
    # This would normally be answered by the Streamlit chat interface,
    # but here we'll just verify that the concepts are now "known"
    detect_and_log_unknown_words(q3)
    
    unknowns = [u.id for u in get_unknown_words()]
    logger.info(f"Unknowns in Q3: {unknowns}")
    
    # In a real scenario, we'd check if the model now produces better answers
    # For this test, we verify that fewer new unknowns are detected
    print(f"‚úÖ Q3 analysis complete: {len(unknowns)} unknowns detected")
    print(f"   (Fewer than Q1 indicates learning is being reused)")

def test_rethink_feedback():
    """Test: Provide feedback, verify rethinking occurs"""
    print_section("TEST 4: Feedback & Rethinking")
    
    wrong_answer = "Neural networks are just a type of decision tree."
    question = "What are neural networks?"
    
    logger.info(f"Original wrong answer: {wrong_answer}")
    logger.info(f"Question: {question}")
    logger.info("Triggering rethink_and_learn()...")
    
    try:
        # Trigger rethinking with feedback
        rethink_result = rethink_and_learn(
            original_prompt=question,
            incorrect_answer=wrong_answer
        )
        
        if rethink_result:
            logger.info(f"Rethink result: {rethink_result[:200]}...")
            print(f"‚úÖ Rethinking triggered successfully")
            print(f"   Corrected answer starts with: {rethink_result[:80]}...")
            return True
        else:
            logger.warning("‚ö†Ô∏è  Rethink returned empty result")
            return False
    except Exception as e:
        logger.error(f"Error in rethinking: {e}")
        return False

def test_deep_learning():
    """Test: Trigger deep learning on learned concepts"""
    print_section("TEST 5: Deep Learning (Optional Advanced Learning)")
    
    logger.info("Triggering deep_learning()...")
    
    try:
        result = deep_learning()
        logger.info(f"Deep learning result: {json.dumps(result, indent=2)}")
        
        learned = result.get('learned_unknown', 0)
        deepened = result.get('deepened_known', 0)
        
        print(f"‚úÖ Deep learning completed:")
        print(f"   New concepts learned: {learned}")
        print(f"   Existing concepts deepened: {deepened}")
        return True
    except Exception as e:
        logger.error(f"Error in deep learning: {e}")
        return False

def main():
    """Run all tests in sequence"""
    print("\n" + "="*70)
    print("  MULTI-TURN LEARNING PIPELINE TEST")
    print("="*70)
    print(f"  Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = {}
    
    try:
        # Test 1: Unknown Detection
        t1_start = time.time()
        q1, unknowns = test_q1_detection()
        results['test1_detection'] = {
            'status': 'passed',
            'time_sec': time.time() - t1_start,
            'unknowns_found': len(unknowns)
        }
    except Exception as e:
        logger.error(f"Test 1 failed: {e}")
        results['test1_detection'] = {'status': 'failed', 'error': str(e)}
    
    try:
        # Test 2: Quick Learning
        t2_start = time.time()
        success = test_q2_learning()
        results['test2_learning'] = {
            'status': 'passed' if success else 'warning',
            'time_sec': time.time() - t2_start
        }
    except Exception as e:
        logger.error(f"Test 2 failed: {e}")
        results['test2_learning'] = {'status': 'failed', 'error': str(e)}
    
    try:
        # Test 3: Learning Verification
        t3_start = time.time()
        test_q3_improvement()
        results['test3_improvement'] = {
            'status': 'passed',
            'time_sec': time.time() - t3_start
        }
    except Exception as e:
        logger.error(f"Test 3 failed: {e}")
        results['test3_improvement'] = {'status': 'failed', 'error': str(e)}
    
    try:
        # Test 4: Rethinking
        t4_start = time.time()
        success = test_rethink_feedback()
        results['test4_rethink'] = {
            'status': 'passed' if success else 'warning',
            'time_sec': time.time() - t4_start
        }
    except Exception as e:
        logger.error(f"Test 4 failed: {e}")
        results['test4_rethink'] = {'status': 'failed', 'error': str(e)}
    
    try:
        # Test 5: Deep Learning (optional)
        t5_start = time.time()
        success = test_deep_learning()
        results['test5_deep'] = {
            'status': 'passed' if success else 'warning',
            'time_sec': time.time() - t5_start
        }
    except Exception as e:
        logger.error(f"Test 5 failed: {e}")
        results['test5_deep'] = {'status': 'failed', 'error': str(e)}
    
    # Summary
    print_section("TEST SUMMARY")
    
    passed = sum(1 for r in results.values() if r.get('status') == 'passed')
    warning = sum(1 for r in results.values() if r.get('status') == 'warning')
    failed = sum(1 for r in results.values() if r.get('status') == 'failed')
    
    for test_name, result in results.items():
        status_emoji = "‚úÖ" if result['status'] == 'passed' else "‚ö†Ô∏è " if result['status'] == 'warning' else "‚ùå"
        time_str = f" ({result.get('time_sec', 0):.2f}s)" if result.get('time_sec') else ""
        print(f"{status_emoji} {test_name}: {result['status']}{time_str}")
    
    print(f"\nüìä Results: {passed} passed, {warning} warnings, {failed} failed")
    print(f"   Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if failed == 0:
        print("\nüéâ All core tests passed! Learning pipeline is working.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {failed} test(s) failed. See details above.")
        return 1

if __name__ == '__main__':
    sys.exit(main())
